{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad7981c-1b80-4666-ba58-38c7047daa87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## RLHF (Reinforcement Learning with Human Feedback) - from scratch <br><br>\n",
    "\n",
    "Inspired by the diy \"min\" versions of AI models, this is a bare bones implementation of applying RLFH to align a language model with human preferences.<br>\n",
    "The focus is on demonstrating key concepts involved in the training of large language models<br>\n",
    "\n",
    "This skips the supervised fine-tuning step typically used in LLMS.\n",
    "\n",
    "There are 4 main parts:\n",
    "\n",
    "- Part 1: Language model to generate english words\n",
    "- Part 2: Train a reward model on human preferences (generate short nouns)\n",
    "- Part 3: Train a policy gradient model to align the language model with your preferences\n",
    "- Part 4: Train a PPO model to align the language model with your preferences\n",
    " \n",
    "Ref: https://github.com/thomfoster/minRLHF, https://github.com/karpathy/minGPT, https://www.reddit.com/r/LocalLLaMA/comments/158l4f0/andrei_karpathys_nanogpt_the_missing_lecture/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2763456-74c0-427a-b140-52675f8779b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 1: Language model to generate english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9690b88-b3e1-43a5-a929-12308825cfaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import torch\n",
    "import numpy as np\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3802d747-a4fb-4ebe-83fd-170e7b2099d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# tokenisation\n",
    "\n",
    "WORD_START_TOKEN = '^'\n",
    "WORD_END_TOKEN = '$'\n",
    "WINDOW_LENGTH = 5 # GPT 3.5: 4,096, GPT 4: 32,768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f609bb5c-3f0f-49fc-93c2-29777bc130cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 9474\n",
      "Average word length: 6.802301034409964\n",
      "['product', 'system', 'post', 'her', 'city', 'add', 'policy', 'number', 'such', 'please']\n"
     ]
    }
   ],
   "source": [
    "# import a list of ~10k most common words on the internet\n",
    "\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-no-swears.txt') as f:\n",
    "  word_list = f.read().decode()\n",
    "word_list = word_list.split('\\n')\n",
    "word_list = [word for word in word_list if len(word)>=3]\n",
    "\n",
    "word_set = set(word_list)\n",
    "\n",
    "print(\"Number of words:\", len(word_list))\n",
    "print(\"Average word length:\", sum([len(word) for word in word_list])/len(word_list))\n",
    "\n",
    "print(word_list[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a889ba1b-cdb1-475f-8543-eb42fe62501e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "tokens = ''.join(sorted({token for token in ''.join(word_list)})) + WORD_START_TOKEN + WORD_END_TOKEN # string of tokens in the corpus\n",
    "stoi = dict(zip(tokens, range(len(tokens)))) # char to int conversion\n",
    "atoi = dict(zip(range(len(tokens)),tokens) )# reverse map of atoi\n",
    "\n",
    "encoder = lambda word: [stoi[s] for s in word] # encode a string into integer tokens: \"abc\" -> [0, 1, 2]\n",
    "decoder = lambda chars: \"\".join(atoi[i] for i in chars) # decode an encoded string\n",
    "\n",
    "stot = lambda word: torch.nn.functional.one_hot(torch.tensor(encoder(word)), len(tokens)) # get the one hot encoding representation of a word\n",
    "otts = lambda x: decoder(x.argmax(-1).tolist()) # one hot encoding of a word to string\n",
    "\n",
    "flatten = lambda matrix: [item for row in matrix for item in row] # flatten a 2d list\n",
    "\n",
    "gen_pairs = lambda word, window_length: ([((WORD_START_TOKEN * window_length) + word)[i:i+window_length] for i in range(len(word) + 1)],[((WORD_START_TOKEN * window_length) + word)[i+window_length] for i in range(len(word))] + [WORD_END_TOKEN]) # turns a word into training pairs for the language model\n",
    "\n",
    "\n",
    "def gen_samples(model, num_samples = 20, max_word_size = 20): # print text samples out of a nn\n",
    "  samples = []\n",
    "  with torch.no_grad():\n",
    "    for _ in range(1000):\n",
    "      sample = torch.stack([stot(WINDOW_LENGTH * WORD_START_TOKEN).argmax(-1).to(device)]).clone()\n",
    "      \n",
    "      for _ in range(max_word_size):\n",
    "        logits = model(sample[:, -5:]) # empty prompt\n",
    "        probs = torch.nn.functional.softmax(logits, -1)\n",
    "        tok = torch.multinomial(probs, 1)\n",
    "\n",
    "        if atoi[tok.item()] == WORD_END_TOKEN:\n",
    "          break\n",
    "        sample = torch.cat([sample, tok], 1)\n",
    "\n",
    "      sample = decoder(sample.squeeze().tolist()[WINDOW_LENGTH:])\n",
    "      samples.append(sample)\n",
    "    print(f\"samples:  {samples[:num_samples]}\")\n",
    "    print(f\"avg word length:  {sum([len(sample) for sample in samples])/len(samples)}\")\n",
    "    print(f\"num valid english words (out of 1000): {len(set(samples).intersection(word_set))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0ce124-b71e-4632-82ac-6164d84124e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('^^^^^', 's'), ('^^^^s', 'e'), ('^^^se', 'a'), ('^^sea', 'r'), ('^sear', 'c'), ('searc', 'h'), ('earch', '$'), ('^^^^^', 'f'), ('^^^^f', 'r'), ('^^^fr', 'e'), ('^^fre', 'e'), ('^free', '$'), ('^^^^^', 'b'), ('^^^^b', 'u'), ('^^^bu', 't'), ('^^but', '$'), ('^^^^^', 'o'), ('^^^^o', 'u'), ('^^^ou', 'r'), ('^^our', '$'), ('^^^^^', 'o'), ('^^^^o', 'n'), ('^^^on', 'e'), ('^^one', '$'), ('^^^^^', 'o'), ('^^^^o', 't'), ('^^^ot', 'h'), ('^^oth', 'e'), ('^othe', 'r'), ('other', '$'), ('^^^^^', 'i'), ('^^^^i', 'n'), ('^^^in', 'f'), ('^^inf', 'o'), ('^info', 'r'), ('infor', 'm'), ('nform', 'a'), ('forma', 't'), ('ormat', 'i'), ('rmati', 'o'), ('matio', 'n'), ('ation', '$'), ('^^^^^', 't'), ('^^^^t', 'i'), ('^^^ti', 'm'), ('^^tim', 'e'), ('^time', '$'), ('^^^^^', 't'), ('^^^^t', 'h'), ('^^^th', 'e'), ('^^the', 'y'), ('^they', '$'), ('^^^^^', 's'), ('^^^^s', 'i'), ('^^^si', 't'), ('^^sit', 'e'), ('^site', '$'), ('^^^^^', 'm'), ('^^^^m', 'a'), ('^^^ma', 'y'), ('^^may', '$'), ('^^^^^', 'w'), ('^^^^w', 'h'), ('^^^wh', 'a'), ('^^wha', 't'), ('^what', '$'), ('^^^^^', 'w'), ('^^^^w', 'h'), ('^^^wh', 'i'), ('^^whi', 'c'), ('^whic', 'h'), ('which', '$'), ('^^^^^', 't'), ('^^^^t', 'h'), ('^^^th', 'e'), ('^^the', 'i'), ('^thei', 'r'), ('their', '$'), ('^^^^^', 'n'), ('^^^^n', 'e'), ('^^^ne', 'w'), ('^^new', 's'), ('^news', '$'), ('^^^^^', 'o'), ('^^^^o', 'u'), ('^^^ou', 't'), ('^^out', '$'), ('^^^^^', 'u'), ('^^^^u', 's'), ('^^^us', 'e'), ('^^use', '$'), ('^^^^^', 'a'), ('^^^^a', 'n'), ('^^^an', 'y'), ('^^any', '$'), ('^^^^^', 't'), ('^^^^t', 'h'), ('^^^th', 'e'), ('^^the', 'r'), ('^ther', 'e')]\n",
      "torch.Size([73919, 5, 28]) torch.Size([73919, 1, 28])\n"
     ]
    }
   ],
   "source": [
    "pairs = [gen_pairs(word, WINDOW_LENGTH) for word in word_list]\n",
    "X = torch.stack(flatten([[stot(p) for p in pair[0]] for pair in pairs])).to(device)\n",
    "y = torch.stack(flatten([[stot(p) for p in pair[1]] for pair in pairs])).to(device)\n",
    "\n",
    "print([(otts(X[i]), otts(y[i])) for i in range(100,200)])\n",
    "\n",
    "# shuffle data\n",
    "idx = torch.randperm(len(y))\n",
    "X, y = X[idx], y[idx]\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa5e1c1-4a9a-499d-b7c4-dc11687509b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([66527, 5, 28]),\n",
       " torch.Size([3696, 5, 28]),\n",
       " torch.Size([3696, 5, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train, test, validation split\n",
    "\n",
    "val_idx, test_idx = int(len(X) * .90), int(len(X) * .95)\n",
    "\n",
    "X_train, y_train = X[:val_idx], y[:val_idx]\n",
    "X_val, y_val = X[val_idx:test_idx], y[val_idx:test_idx]\n",
    "X_test, y_test = X[test_idx:], y[test_idx:]\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc1b4fd-b386-4642-835b-47fb2b3e73c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharModel(\n",
      "  (tok_emb): Embedding(28, 64)\n",
      "  (pos_emb): Embedding(5, 64)\n",
      "  (kw): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (qw): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (vw): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (mhsa): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (mhsa_ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (relu): ReLU()\n",
      "  (mhsa_head): Linear(in_features=320, out_features=28, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CharModel(torch.nn.Module):\n",
    "  def __init__(self, tokens_sz, ctx_sz, emb_sz, head_sz, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.tok_emb = torch.nn.Embedding(tokens_sz, emb_sz, device = device)\n",
    "    self.pos_emb = torch.nn.Embedding(ctx_sz, emb_sz, device = device)\n",
    "    self.pos_idx = torch.arange(ctx_sz, device = device)\n",
    "\n",
    "    self.kw = torch.nn.Linear(emb_sz, head_sz, device = device, bias = False)\n",
    "    self.qw = torch.nn.Linear(emb_sz, head_sz, device = device, bias = False)\n",
    "    self.vw = torch.nn.Linear(emb_sz, head_sz, device = device, bias = False)\n",
    "    self.mhsa = torch.nn.MultiheadAttention(head_sz, 2, batch_first = True, device = device)\n",
    "    self.mhsa_ln = torch.nn.LayerNorm(head_sz, device = device)\n",
    "    self.flatten = torch.nn.Flatten(1)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.mhsa_head = torch.nn.Linear(head_sz * ctx_sz, tokens_sz, device = device)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.tok_emb(x) + self.pos_emb(self.pos_idx)\n",
    "    q, k, v = self.qw(x), self.kw(x), self.vw(x)\n",
    "    x, _ = self.mhsa(q, k, v,  )\n",
    "    x = self.mhsa_ln(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.mhsa_head(x)\n",
    "    return x\n",
    "  \n",
    "\n",
    "EMBEDDING_SIZE = 64\n",
    "MULTI_HEAD_ATTENTION_SIZE = 64\n",
    "\n",
    "nn = CharModel(len(tokens), WINDOW_LENGTH, EMBEDDING_SIZE, MULTI_HEAD_ATTENTION_SIZE, device)\n",
    "model = deepcopy(nn)\n",
    "optim = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "print(model) # learn what a NonDynamicallyQuantizableLinear is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc52a7a-37b7-493f-ad3e-60acc051fa7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 train loss 3.3967 val loss 3.3911\n",
      "samples:  ['hsdqqfpqlhcsoorxtlwo', 'jsjtiiqpkgjckyrtwgac', 'uhvhx', 'prvh^njhsxytg', 'rpjpizvxvijdqwpqtax', 'inromwirtvvornlg^xmu', 'wawmddwwvtuxeo^rpslf', 'f^vsvfehwi^ejhiuqusg', 'agg^h', 'zsrkzu^k^kkogxl^hubp', 'zii^s^sdakeh', 'ffoo', 'rrhvhhuhkqjwewwbsjg^', 'hpqaqhevytmowdxgsoom', 'hlfmdtlhqqmshzxv^kvd', 'lhqdoqtkx^hlcx', 'yiehbuhxoov^', 'dsr^elhwdx^w', 'aasdyhatwdttfmviewro', 'gafiqjbd']\n",
      "avg word length:  14.757\n",
      "num valid english words (out of 1000): 0\n",
      "\n",
      "epoch 100 train loss 2.1979 val loss 2.2066\n",
      "samples:  ['edimorion', 'suberes', 'comnopperts', 'rendueds', 'ressedind', 'treameninc', 'falled', 'saking', 'igeeptar', 'rinks', 'acor', 'indist', 'rptors', 'dealre', 'refere', 'indes', 'drenteres', 'pex', 'tote', 'girkess']\n",
      "avg word length:  6.832\n",
      "num valid english words (out of 1000): 37\n",
      "\n",
      "epoch 200 train loss 2.0880 val loss 2.1339\n",
      "samples:  ['extercinging', 'hoxplloled', 'shork', 'astager', 'cootands', 'susercy', 'gre', 'exp', 'dick', 'ins', 'quens', 'obpsly', 'porlativies', 'desciand', 'agres', 'pritune', 'zadearme', 'morts', 'revienc', 'menity']\n",
      "avg word length:  6.586\n",
      "num valid english words (out of 1000): 70\n",
      "\n",
      "epoch 300 train loss 2.0330 val loss 2.1047\n",
      "samples:  ['delt', 'relles', 'warying', 'painfort', 'trang', 'ambui', 'reacters', 'stmis', 'okimes', 'goach', 'conscreds', 'mastal', 'stop', 'opese', 'spriges', 'lanilist', 'forgostan', 'light', 'talihand', 'vaynighton']\n",
      "avg word length:  6.722\n",
      "num valid english words (out of 1000): 71\n",
      "\n",
      "epoch 400 train loss 1.9959 val loss 2.0880\n",
      "samples:  ['thar', 'inrom', 'ori', 'sersw', 'daq', 'dielum', 'ensevering', 'rearals', 'farys', 'markfasian', 'lisminita', 'opic', 'cenna', 'aite', 'implited', 'josh', 'endication', 'fealy', 'betor', 'gracters']\n",
      "avg word length:  6.842\n",
      "num valid english words (out of 1000): 66\n",
      "\n",
      "epoch 500 train loss 1.9676 val loss 2.0791\n",
      "samples:  ['drave', 'artionst', 'weich', 'subrescrest', 'ask', 'hold', 'elind', 'tegary', 'shothing', 'mossistalless', 'acted', 'prig', 'feckn', 'hace', 'orclatulated', 'intelding', 'col', 'treal', 'sienling', 'spacy']\n",
      "avg word length:  6.653\n",
      "num valid english words (out of 1000): 91\n",
      "\n",
      "epoch 600 train loss 1.9443 val loss 2.0736\n",
      "samples:  ['jasueng', 'verry', 'warnable', 'su', 'warm', 'dosing', 'ladens', 'docing', 'mirchest', 'delig', 'relector', 'tetoria', 'memer', 'etent', 'colo', 'oved', 'arubden', 'lange', 'bear', 'eacelly']\n",
      "avg word length:  6.667\n",
      "num valid english words (out of 1000): 90\n",
      "\n",
      "epoch 700 train loss 1.9241 val loss 2.0642\n",
      "samples:  ['dayordation', 'shella', 'desking', 'adeson', 'optiation', 'lited', 'clabila', 'perbetin', 'marourteeragest', 'wolded', 'fallowre', 'crimaind', 'havisferise', 'camile', 'ungominial', 'chore', 'bar', 'seulely', 'notarminuariation', 'jeura']\n",
      "avg word length:  6.738\n",
      "num valid english words (out of 1000): 96\n",
      "\n",
      "epoch 800 train loss 1.9069 val loss 2.0614\n",
      "samples:  ['mayate', 'fage', 'untor', 'uperars', 'leb', 'remboand', 'auncome', 'qeutess', 'advams', 'firhing', 'urbolivideed', 'exampters', 'sostren', 'cash', 'lever', 'peneration', 'pop', 'opprouturt', 'holdy', 'metep']\n",
      "avg word length:  6.876\n",
      "num valid english words (out of 1000): 100\n",
      "\n",
      "epoch 900 train loss 1.8929 val loss 2.0612\n",
      "samples:  ['lagra', 'vchnerby', 'lousder', 'alg', 'vision', 'drai', 'timoland', 'polislinit', 'tremic', 'com', 'amen', 'questoph', 'gelievestic', 'complacein', 'lecormentrea', 'brane', 'arpounte', 'yimed', 'algate', 'eskil']\n",
      "avg word length:  6.793\n",
      "num valid english words (out of 1000): 92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    "  X_batch, y_batch = X_train.argmax(-1), torch.squeeze(y_train.argmax(-1))\n",
    "  logits = model(X_batch)\n",
    "  loss = torch.nn.functional.cross_entropy(logits, y_batch)\n",
    "  if epoch % 100 == 0:\n",
    "    with torch.no_grad():\n",
    "      val_loss = torch.nn.functional.cross_entropy(model(X_val.argmax(-1)), torch.squeeze(y_val.argmax(-1)))\n",
    "      print(f\"epoch {epoch:2d} train loss {loss.item():.4f} val loss {val_loss.item():.4f}\")\n",
    "      gen_samples(model)\n",
    "      print()\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  optim.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9944d1f-25b6-4985-bf1e-cebdbeaed696",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 2: Train a reward model on human preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf3c951e-4207-4de9-b017-36eafe0f888f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'site',\n",
       " 'news',\n",
       " 'use',\n",
       " 'view',\n",
       " 'date',\n",
       " 'name',\n",
       " 'year',\n",
       " 'day',\n",
       " 'work',\n",
       " 'data',\n",
       " 'city',\n",
       " 'add',\n",
       " 'book',\n",
       " 'user',\n",
       " 'mail',\n",
       " 'life',\n",
       " 'way',\n",
       " 'days',\n",
       " 'part',\n",
       " 'item',\n",
       " 'ebay',\n",
       " 'line',\n",
       " 'type',\n",
       " 'car',\n",
       " 'take',\n",
       " 'area',\n",
       " 'dvd',\n",
       " 'code',\n",
       " 'show',\n",
       " 'link',\n",
       " 'case',\n",
       " 'game',\n",
       " 'care',\n",
       " 'end',\n",
       " 'law',\n",
       " 'size',\n",
       " 'shop',\n",
       " 'text',\n",
       " 'rate',\n",
       " 'form',\n",
       " 'john',\n",
       " 'call',\n",
       " 'non',\n",
       " 'york',\n",
       " 'man',\n",
       " 'card',\n",
       " 'jobs',\n",
       " 'food',\n",
       " 'sale',\n",
       " 'job',\n",
       " 'room',\n",
       " 'join',\n",
       " 'men',\n",
       " 'box',\n",
       " 'week',\n",
       " 'note',\n",
       " 'air',\n",
       " 'plan',\n",
       " 'test',\n",
       " 'dec',\n",
       " 'cart',\n",
       " 'tax',\n",
       " 'blog',\n",
       " 'let',\n",
       " 'park',\n",
       " 'act',\n",
       " 'body',\n",
       " 'age',\n",
       " 'road',\n",
       " 'gift',\n",
       " 'war',\n",
       " 'nov',\n",
       " 'fax',\n",
       " 'star',\n",
       " 'hand',\n",
       " 'rss',\n",
       " 'net',\n",
       " 'term',\n",
       " 'film',\n",
       " 'head',\n",
       " 'cell',\n",
       " 'self',\n",
       " 'cars',\n",
       " 'fun',\n",
       " 'gold',\n",
       " 'feb',\n",
       " 'arts',\n",
       " 'lot',\n",
       " 'mar',\n",
       " 'land',\n",
       " 'word',\n",
       " 'bill',\n",
       " 'kids',\n",
       " 'rock',\n",
       " 'tips',\n",
       " 'auto',\n",
       " 'fact',\n",
       " 'unit',\n",
       " 'tech',\n",
       " 'meet',\n",
       " 'feel',\n",
       " 'bank',\n",
       " 'risk',\n",
       " 'jul',\n",
       " 'town',\n",
       " 'jun',\n",
       " 'girl',\n",
       " 'golf',\n",
       " 'loan',\n",
       " 'sort',\n",
       " 'half',\n",
       " 'none',\n",
       " 'paul',\n",
       " 'sony',\n",
       " 'html',\n",
       " 'loss',\n",
       " 'face',\n",
       " 'oil',\n",
       " 'bit',\n",
       " 'base',\n",
       " 'turn',\n",
       " 'drug',\n",
       " 'pics',\n",
       " 'cash',\n",
       " 'bay',\n",
       " 'port',\n",
       " 'stop',\n",
       " 'bar',\n",
       " 'dog',\n",
       " 'mind',\n",
       " 'pdf',\n",
       " 'menu',\n",
       " 'role',\n",
       " 'mon',\n",
       " 'com',\n",
       " 'hour',\n",
       " 'gas',\n",
       " 'sat',\n",
       " 'bid',\n",
       " 'kind',\n",
       " 'move',\n",
       " 'logo',\n",
       " 'band',\n",
       " 'lead',\n",
       " 'fri',\n",
       " 'mode',\n",
       " 'fund',\n",
       " 'song',\n",
       " 'cnet',\n",
       " 'ltd',\n",
       " 'fall',\n",
       " 'idea',\n",
       " 'inc',\n",
       " 'tool',\n",
       " 'bed',\n",
       " 'hill',\n",
       " 'deal',\n",
       " 'tue',\n",
       " 'feed',\n",
       " 'sea',\n",
       " 'cut',\n",
       " 'hall',\n",
       " 'tel',\n",
       " 'ship',\n",
       " 'hair',\n",
       " 'kit',\n",
       " 'tree',\n",
       " 'boy',\n",
       " 'vote',\n",
       " 'ways',\n",
       " 'son',\n",
       " 'rule',\n",
       " 'gmt',\n",
       " 'xml',\n",
       " 'feet',\n",
       " 'bin',\n",
       " 'cool',\n",
       " 'asia',\n",
       " 'java',\n",
       " 'pass',\n",
       " 'van',\n",
       " 'fees',\n",
       " 'prev',\n",
       " 'ads',\n",
       " 'ring',\n",
       " 'int',\n",
       " 'iraq',\n",
       " 'rest',\n",
       " 'pool',\n",
       " 'mini',\n",
       " 'eye',\n",
       " 'pack',\n",
       " 'race',\n",
       " 'debt',\n",
       " 'sets',\n",
       " 'wood',\n",
       " 'msn',\n",
       " 'fee',\n",
       " 'dark',\n",
       " 'aid',\n",
       " 'host',\n",
       " 'saw',\n",
       " 'mike',\n",
       " 'trip',\n",
       " 'pst',\n",
       " 'eyes',\n",
       " 'tom',\n",
       " 'sub',\n",
       " 'hear',\n",
       " 'wife',\n",
       " 'ten',\n",
       " 'cat',\n",
       " 'jack',\n",
       " 'flow',\n",
       " 'path',\n",
       " 'laws',\n",
       " 'guy',\n",
       " 'cup',\n",
       " 'vol',\n",
       " 'army',\n",
       " 'lots',\n",
       " 'firm',\n",
       " 'dvds',\n",
       " 'ball',\n",
       " 'goal',\n",
       " 'wind',\n",
       " 'palm',\n",
       " 'bob',\n",
       " 'fit',\n",
       " 'xbox',\n",
       " 'ford',\n",
       " 'root',\n",
       " 'ice',\n",
       " 'llc',\n",
       " 'bus',\n",
       " 'cold',\n",
       " 'mass',\n",
       " 'heat',\n",
       " 'task',\n",
       " 'bug',\n",
       " 'fuel',\n",
       " 'walk',\n",
       " 'jim',\n",
       " 'tags',\n",
       " 'joe',\n",
       " 'guys',\n",
       " 'drop',\n",
       " 'ones',\n",
       " 'rank',\n",
       " 'inch',\n",
       " 'lab',\n",
       " 'camp',\n",
       " 'fill',\n",
       " 'fort',\n",
       " 'gene',\n",
       " 'disc',\n",
       " 'boat',\n",
       " 'icon',\n",
       " 'cast',\n",
       " 'aids',\n",
       " 'flag',\n",
       " 'iron',\n",
       " 'void',\n",
       " 'tag',\n",
       " 'mix',\n",
       " 'disk',\n",
       " 'vhs',\n",
       " 'desk',\n",
       " 'vice',\n",
       " 'ray',\n",
       " 'duty',\n",
       " 'gain',\n",
       " 'iowa',\n",
       " 'spa',\n",
       " 'con',\n",
       " 'ups',\n",
       " 'zoom',\n",
       " 'blow',\n",
       " 'wire',\n",
       " 'tape',\n",
       " 'spam',\n",
       " 'cent',\n",
       " 'zero',\n",
       " 'roll',\n",
       " 'bath',\n",
       " 'beta',\n",
       " 'jazz',\n",
       " 'bags',\n",
       " 'mom',\n",
       " 'bars',\n",
       " 'row',\n",
       " 'rise',\n",
       " 'bird',\n",
       " 'fans',\n",
       " 'eat',\n",
       " 'dell',\n",
       " 'seat',\n",
       " 'aim',\n",
       " 'bids',\n",
       " 'toll',\n",
       " 'cape',\n",
       " 'tip',\n",
       " 'mine',\n",
       " 'math',\n",
       " 'dogs',\n",
       " 'bbc',\n",
       " 'arms',\n",
       " 'tea',\n",
       " 'sky',\n",
       " 'utah',\n",
       " 'hide',\n",
       " 'toy',\n",
       " 'hip',\n",
       " 'spot',\n",
       " 'dot',\n",
       " 'hiv',\n",
       " 'pda',\n",
       " 'rain',\n",
       " 'dna',\n",
       " 'diff',\n",
       " 'bass',\n",
       " 'hole',\n",
       " 'pets',\n",
       " 'sql',\n",
       " 'pair',\n",
       " 'evil',\n",
       " 'gps',\n",
       " 'cap',\n",
       " 'ink',\n",
       " 'bell',\n",
       " 'gnu',\n",
       " 'jeff',\n",
       " 'lane',\n",
       " 'ages',\n",
       " 'cook',\n",
       " 'perl',\n",
       " 'bike',\n",
       " 'seek',\n",
       " 'tony',\n",
       " 'kits',\n",
       " 'cam',\n",
       " 'soil',\n",
       " 'wet',\n",
       " 'ram',\n",
       " 'exit',\n",
       " 'arm',\n",
       " 'keys',\n",
       " 'acts',\n",
       " 'poll',\n",
       " 'bond',\n",
       " 'hop',\n",
       " 'visa',\n",
       " 'pure',\n",
       " 'draw',\n",
       " 'warm',\n",
       " 'babe',\n",
       " 'crew',\n",
       " 'node',\n",
       " 'lock',\n",
       " 'mile',\n",
       " 'kid',\n",
       " 'pan',\n",
       " 'dish',\n",
       " 'adam',\n",
       " 'slot',\n",
       " 'demo',\n",
       " 'hate',\n",
       " 'rice',\n",
       " 'vary',\n",
       " 'milk',\n",
       " 'push',\n",
       " 'alan',\n",
       " 'oak',\n",
       " 'vat',\n",
       " 'beer',\n",
       " 'jose',\n",
       " 'jane',\n",
       " 'sir',\n",
       " 'twin',\n",
       " 'bits',\n",
       " 'suit',\n",
       " 'chip',\n",
       " 'wow',\n",
       " 'que',\n",
       " 'fig',\n",
       " 'nick',\n",
       " 'plot',\n",
       " 'qty',\n",
       " 'pump',\n",
       " 'anne',\n",
       " 'beds',\n",
       " 'pcs',\n",
       " 'bulk',\n",
       " 'sci',\n",
       " 'edt',\n",
       " 'sin',\n",
       " 'cute',\n",
       " 'para',\n",
       " 'seed',\n",
       " 'meat',\n",
       " 'alex',\n",
       " 'bone',\n",
       " 'ftp',\n",
       " 'tone',\n",
       " 'neck',\n",
       " 'rail',\n",
       " 'jay',\n",
       " 'belt',\n",
       " 'rob',\n",
       " 'era',\n",
       " 'asp',\n",
       " 'mad',\n",
       " 'semi',\n",
       " 'exp',\n",
       " 'till',\n",
       " 'shoe',\n",
       " 'sand',\n",
       " 'joy',\n",
       " 'ran',\n",
       " 'lies',\n",
       " 'deck',\n",
       " 'mph',\n",
       " 'dose',\n",
       " 'bet',\n",
       " 'cats',\n",
       " 'nba',\n",
       " 'greg',\n",
       " 'ron',\n",
       " 'folk',\n",
       " 'org',\n",
       " 'lift',\n",
       " 'mall',\n",
       " 'dad',\n",
       " 'pat',\n",
       " 'reg',\n",
       " 'tion',\n",
       " 'dust',\n",
       " 'wiki',\n",
       " 'kent',\n",
       " 'ear',\n",
       " 'ward',\n",
       " 'yoga',\n",
       " 'ceo',\n",
       " 'glad',\n",
       " 'rack',\n",
       " 'rep',\n",
       " 'mit',\n",
       " 'boss',\n",
       " 'ross',\n",
       " 'solo',\n",
       " 'tall',\n",
       " 'pdas',\n",
       " 'sri',\n",
       " 'api',\n",
       " 'lan',\n",
       " 'sms',\n",
       " 'drum',\n",
       " 'foto',\n",
       " 'ease',\n",
       " 'tabs',\n",
       " 'tend',\n",
       " 'gulf',\n",
       " 'rick',\n",
       " 'hunt',\n",
       " 'mill',\n",
       " 'aud',\n",
       " 'burn',\n",
       " 'gon',\n",
       " 'amp',\n",
       " 'clay',\n",
       " 'amd',\n",
       " 'wise',\n",
       " 'odds',\n",
       " 'marc',\n",
       " 'sons',\n",
       " 'rod',\n",
       " 'cuba',\n",
       " 'hrs',\n",
       " 'kate',\n",
       " 'wolf',\n",
       " 'slip',\n",
       " 'rpm',\n",
       " 'cuts',\n",
       " 'mars',\n",
       " 'tvs',\n",
       " 'egg',\n",
       " 'mhz',\n",
       " 'pill',\n",
       " 'meta',\n",
       " 'mint',\n",
       " 'spin',\n",
       " 'wash',\n",
       " 'aims',\n",
       " 'soap',\n",
       " 'jam',\n",
       " 'guns',\n",
       " 'rio',\n",
       " 'hero',\n",
       " 'punk',\n",
       " 'duke',\n",
       " 'pace',\n",
       " 'wage',\n",
       " 'dawn',\n",
       " 'carl',\n",
       " 'coat',\n",
       " 'mrs',\n",
       " 'rica',\n",
       " 'app',\n",
       " 'ion',\n",
       " 'doll',\n",
       " 'reed',\n",
       " 'mice',\n",
       " 'ban',\n",
       " 'temp',\n",
       " 'wrap',\n",
       " 'beam',\n",
       " 'tops',\n",
       " 'amy',\n",
       " 'shut',\n",
       " 'ncaa',\n",
       " 'thou',\n",
       " 'phd',\n",
       " 'mask',\n",
       " 'coal',\n",
       " 'cry',\n",
       " 'zoo',\n",
       " 'tee',\n",
       " 'lion',\n",
       " 'beef',\n",
       " 'cad',\n",
       " 'hats',\n",
       " 'tcp',\n",
       " 'hook',\n",
       " 'cord',\n",
       " 'val',\n",
       " 'crop',\n",
       " 'ghz',\n",
       " 'hub',\n",
       " 'tons',\n",
       " 'sue',\n",
       " 'hang',\n",
       " 'gbp',\n",
       " 'hood',\n",
       " 'chi',\n",
       " 'fame',\n",
       " 'rfc',\n",
       " 'ins',\n",
       " 'jpg',\n",
       " 'mins',\n",
       " 'stem',\n",
       " 'opt',\n",
       " 'tune',\n",
       " 'corn',\n",
       " 'ties',\n",
       " 'rat',\n",
       " 'brad',\n",
       " 'jury',\n",
       " 'dos',\n",
       " 'tail',\n",
       " 'lawn',\n",
       " 'soup',\n",
       " 'byte',\n",
       " 'msg',\n",
       " 'cod',\n",
       " 'thru',\n",
       " 'jews',\n",
       " 'gen',\n",
       " 'quit',\n",
       " 'wan',\n",
       " 'tale',\n",
       " 'std',\n",
       " 'coin',\n",
       " 'fda',\n",
       " 'arch',\n",
       " 'hdtv',\n",
       " 'asin',\n",
       " 'bomb',\n",
       " 'harm',\n",
       " 'thy',\n",
       " 'pal',\n",
       " 'noon',\n",
       " 'gym',\n",
       " 'cams',\n",
       " 'joel',\n",
       " 'tan',\n",
       " 'mate',\n",
       " 'chef',\n",
       " 'alt',\n",
       " 'pie',\n",
       " 'pete',\n",
       " 'spec',\n",
       " 'bow',\n",
       " 'penn',\n",
       " 'midi',\n",
       " 'dale',\n",
       " 'oils',\n",
       " 'lang',\n",
       " 'stud',\n",
       " 'pos',\n",
       " 'phys',\n",
       " 'pole',\n",
       " 'bend',\n",
       " 'moms',\n",
       " 'cab',\n",
       " 'ist',\n",
       " 'lips',\n",
       " 'cnn',\n",
       " 'lil',\n",
       " 'tire',\n",
       " 'chad',\n",
       " 'josh',\n",
       " 'drag',\n",
       " 'scsi',\n",
       " 'dns',\n",
       " 'pty',\n",
       " 'nuts',\n",
       " 'span',\n",
       " 'sox',\n",
       " 'tub',\n",
       " 'pads',\n",
       " 'cups',\n",
       " 'jvc',\n",
       " 'poem',\n",
       " 'bean',\n",
       " 'bias',\n",
       " 'mem',\n",
       " 'vii',\n",
       " 'rats',\n",
       " 'cfr',\n",
       " 'stat',\n",
       " 'bios',\n",
       " 'ruth',\n",
       " 'pray',\n",
       " 'bare',\n",
       " 'pit',\n",
       " 'mono',\n",
       " 'tile',\n",
       " 'mag',\n",
       " 'gsm',\n",
       " 'ddr',\n",
       " 'knee',\n",
       " 'prep',\n",
       " 'pros',\n",
       " 'sara',\n",
       " 'bra',\n",
       " 'joan',\n",
       " 'phi',\n",
       " 'cet',\n",
       " 'audi',\n",
       " 'volt',\n",
       " 'div',\n",
       " 'dirt',\n",
       " 'acer',\n",
       " 'dist',\n",
       " 'ons',\n",
       " 'sink',\n",
       " 'grip',\n",
       " 'reno',\n",
       " 'polo',\n",
       " 'rpg',\n",
       " 'horn',\n",
       " 'prot',\n",
       " 'diy',\n",
       " 'arg',\n",
       " 'jpeg',\n",
       " 'wal',\n",
       " 'swap',\n",
       " 'abs',\n",
       " 'sim',\n",
       " 'buzz',\n",
       " 'boom',\n",
       " 'rip',\n",
       " 'zope',\n",
       " 'buf',\n",
       " 'sims',\n",
       " 'tray',\n",
       " 'sage',\n",
       " 'bat',\n",
       " 'sap',\n",
       " 'suse',\n",
       " 'wool',\n",
       " 'oops',\n",
       " 'trap',\n",
       " 'dies',\n",
       " 'pts',\n",
       " 'comm',\n",
       " 'lace',\n",
       " 'ste',\n",
       " 'ment',\n",
       " 'rows',\n",
       " 'treo',\n",
       " 'gods',\n",
       " 'tex',\n",
       " 'poly',\n",
       " 'fist',\n",
       " 'cons',\n",
       " 'gis',\n",
       " 'shaw',\n",
       " 'bali',\n",
       " 'judy',\n",
       " 'trio',\n",
       " 'cube',\n",
       " 'gras',\n",
       " 'dis',\n",
       " 'oval',\n",
       " 'href',\n",
       " 'benz',\n",
       " 'earl',\n",
       " 'aus',\n",
       " 'gdp',\n",
       " 'pig',\n",
       " 'mess',\n",
       " 'ada',\n",
       " 'rope',\n",
       " 'foo',\n",
       " 'gba',\n",
       " 'duo',\n",
       " 'yea',\n",
       " 'clan',\n",
       " 'mesa',\n",
       " 'dem',\n",
       " 'wav',\n",
       " 'memo',\n",
       " 'ham',\n",
       " 'reel',\n",
       " 'rand',\n",
       " 'buck',\n",
       " 'sie',\n",
       " 'acre',\n",
       " 'bon',\n",
       " 'len',\n",
       " 'obj',\n",
       " 'dans',\n",
       " 'tent',\n",
       " 'hack',\n",
       " 'dare',\n",
       " 'hawk',\n",
       " 'lamb',\n",
       " 'cos',\n",
       " 'vpn',\n",
       " 'fcc',\n",
       " 'eds',\n",
       " 'junk',\n",
       " 'hans',\n",
       " 'nut',\n",
       " 'sake',\n",
       " 'sans',\n",
       " 'irs',\n",
       " 'bye',\n",
       " 'cdt',\n",
       " 'eau',\n",
       " 'gore',\n",
       " 'cult',\n",
       " 'dash',\n",
       " 'jake',\n",
       " 'eval',\n",
       " 'sao',\n",
       " 'oman',\n",
       " 'gmc',\n",
       " 'rage',\n",
       " 'adsl',\n",
       " 'prix',\n",
       " 'asn',\n",
       " 'acne',\n",
       " 'libs',\n",
       " 'dana',\n",
       " 'halo',\n",
       " 'ppm',\n",
       " 'gays',\n",
       " 'apt',\n",
       " 'exec',\n",
       " 'inf',\n",
       " 'eos',\n",
       " 'maui',\n",
       " 'pct',\n",
       " 'vids',\n",
       " 'yale',\n",
       " 'qld',\n",
       " 'pas',\n",
       " 'doom',\n",
       " 'sas',\n",
       " 'weed',\n",
       " 'oecd',\n",
       " 'dice',\n",
       " 'dock',\n",
       " 'mods',\n",
       " 'hint',\n",
       " 'liz',\n",
       " 'ccd',\n",
       " 'pork',\n",
       " 'boc',\n",
       " 'fare',\n",
       " 'bald',\n",
       " 'leon',\n",
       " 'dame',\n",
       " 'tmp',\n",
       " 'alot',\n",
       " 'fin',\n",
       " 'mud',\n",
       " 'eden',\n",
       " 'incl',\n",
       " 'ala',\n",
       " 'dip',\n",
       " 'reid',\n",
       " 'rosa',\n",
       " 'hash',\n",
       " 'mpg',\n",
       " 'cas',\n",
       " 'dow',\n",
       " 'dui',\n",
       " 'yen',\n",
       " 'worm',\n",
       " 'deaf',\n",
       " 'mats',\n",
       " 'wto',\n",
       " 'hay',\n",
       " 'grad',\n",
       " 'bras',\n",
       " 'gaps',\n",
       " 'ata',\n",
       " 'pam',\n",
       " 'dim',\n",
       " 'idol',\n",
       " 'mai',\n",
       " 'abu',\n",
       " 'cork',\n",
       " 'mali',\n",
       " 'mtv',\n",
       " 'leu',\n",
       " 'sip',\n",
       " 'dee',\n",
       " 'mae',\n",
       " 'mel',\n",
       " 'conf',\n",
       " 'wma',\n",
       " 'cir',\n",
       " 'erik',\n",
       " 'paso',\n",
       " 'norm',\n",
       " 'ips',\n",
       " 'ware',\n",
       " 'mia',\n",
       " 'gtk',\n",
       " 'goat',\n",
       " 'sail',\n",
       " 'dts',\n",
       " 'cdna',\n",
       " 'gage',\n",
       " 'soa',\n",
       " 'urge',\n",
       " 'smtp',\n",
       " 'kurt',\n",
       " 'lone',\n",
       " 'cope',\n",
       " 'lime',\n",
       " 'bool',\n",
       " 'wit',\n",
       " 'bbs',\n",
       " 'spas',\n",
       " 'ind',\n",
       " 'jets',\n",
       " 'intl',\n",
       " 'yarn',\n",
       " 'mug',\n",
       " 'pike',\n",
       " 'ids',\n",
       " 'gzip',\n",
       " 'ctrl',\n",
       " 'bent',\n",
       " 'laos']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Tokenize the words\n",
    "words_tokenized = word_tokenize(\" \".join(word_list))\n",
    "\n",
    "# Perform POS tagging to get the parts of speech for each word\n",
    "pos_tags = pos_tag(words_tokenized)\n",
    "\n",
    "\n",
    "# Filter nouns with length less than 4\n",
    "small_nouns = [word for word, pos in pos_tags if pos.startswith('NN') and len(word)<=4]\n",
    "small_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f92ea89-1cea-42da-b99b-ae03726deb03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871\n",
      "8604\n"
     ]
    }
   ],
   "source": [
    "\n",
    "incentivised_words = small_nouns\n",
    "penalised_words = [word for word in word_list if word not in small_nouns]\n",
    "\n",
    "print(len(incentivised_words))\n",
    "print(len(penalised_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040d4313-a1f2-4076-bccd-b9b14f6886c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "states = []\n",
    "actions = []\n",
    "\n",
    "pairs_incentivised = [gen_pairs(word, WINDOW_LENGTH) for word in incentivised_words]\n",
    "pairs_penalised = [gen_pairs(word, WINDOW_LENGTH) for word in penalised_words]\n",
    "\n",
    "for pair in pairs_incentivised:\n",
    "  pairs.append([pair[0][i] + pair[1][i] for i in range(len(pair[1]))])\n",
    "  states.append([pair[0][i] for i in range(len(pair[1]))])\n",
    "  actions.append([pair[1][i] for i in range(len(pair[1]))])\n",
    "\n",
    "gen_importance = lambda importances: [(importances[j], importances[i]) for i in range(len(importances)) for j in range(i + 1, len(importances))]\n",
    "\n",
    "pairwise_data = []\n",
    "for pair in pairs:\n",
    "  pairwise_data.append(gen_importance(pair))\n",
    "\n",
    "pairwise_data = flatten(pairwise_data)\n",
    "\n",
    "states = flatten(states)\n",
    "actions = flatten(actions)\n",
    "\n",
    "for i in range(ord('a'), ord('n')+1):\n",
    "  pairwise_data.append((WORD_START_TOKEN*(WINDOW_LENGTH)+ chr(i), WORD_START_TOKEN*WINDOW_LENGTH + WORD_END_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e028ca-e7c2-4f93-b9ab-13ab4ea4e5ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('^^^^si', '^^^^^s'),\n",
       " ('^^^sit', '^^^^^s'),\n",
       " ('^^site', '^^^^^s'),\n",
       " ('^site$', '^^^^^s'),\n",
       " ('^^^sit', '^^^^si'),\n",
       " ('^^site', '^^^^si'),\n",
       " ('^site$', '^^^^si'),\n",
       " ('^^site', '^^^sit'),\n",
       " ('^site$', '^^^sit'),\n",
       " ('^site$', '^^site')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_data[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ab8465-7ced-43df-bf4a-52b88fda6752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8a3f357df0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(torch.linspace(-10, 10, 1000), -torch.log(torch.sigmoid(torch.linspace(-10, 10, 1000))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11686a6d-30c0-436d-a799-d443b5c56f5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RewardModel(torch.nn.Module):\n",
    "  def __init__(self, tokens_sz, ctx_sz, emb_sz, head_sz, device):\n",
    "    super().__init__()\n",
    "    ctx_sz += 1\n",
    "\n",
    "    self.tok_emb = torch.nn.Embedding(tokens_sz, emb_sz, device=device)\n",
    "    self.pos_emb = torch.nn.Embedding(ctx_sz, emb_sz, device=device)\n",
    "    self.pos_idx = torch.arange(ctx_sz, device=device)\n",
    "\n",
    "    self.kw = torch.nn.Linear(emb_sz, head_sz, device=device, bias=False)\n",
    "    self.qw = torch.nn.Linear(emb_sz, head_sz, device=device, bias=False)\n",
    "    self.vw = torch.nn.Linear(emb_sz, head_sz, device=device, bias=False)\n",
    "    self.mhsa = torch.nn.MultiheadAttention(head_sz, 2, batch_first=True, device=device)\n",
    "    self.mhsa_ln = torch.nn.LayerNorm(head_sz, device=device)\n",
    "    self.flatten = torch.nn.Flatten(1)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.mhsa_head = torch.nn.Linear(head_sz * ctx_sz, tokens_sz, device=device)\n",
    "    self.logistic_output = torch.nn.Linear(tokens_sz, 1, device=device)  # Add a new linear layer for binary output\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.tok_emb(x) + self.pos_emb(self.pos_idx)\n",
    "      q, k, v = self.qw(x), self.kw(x), self.vw(x)\n",
    "      x, _ = self.mhsa(q, k, v, )\n",
    "      x = self.mhsa_ln(x)\n",
    "      x = self.flatten(x)\n",
    "      x = self.relu(x)\n",
    "      x = self.mhsa_head(x)\n",
    "      x = self.logistic_output(x)  # Apply sigmoid to get the binary output\n",
    "      return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 64\n",
    "MULTI_HEAD_ATTENTION_SIZE = 64\n",
    "\n",
    "      \n",
    "# Custom Bradley Terry Loss\n",
    "class CustomLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y1, y2):\n",
    "        return torch.mean(-torch.log(torch.sigmoid(y1 - y2)))\n",
    "\n",
    "\n",
    "rm = RewardModel(len(tokens), WINDOW_LENGTH, EMBEDDING_SIZE, MULTI_HEAD_ATTENTION_SIZE, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7ad4b6-e575-43fe-bf92-4ef0fa357d5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6720, device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  y1 = rm(torch.stack([stot(pairwise_data[i][0]).argmax(-1) for i in range(len(pairwise_data))]).long().to(device))\n",
    "  y2 = rm(torch.stack([stot(pairwise_data[i][1]).argmax(-1) for i in range(len(pairwise_data))]).long().to(device))\n",
    "\n",
    "  sum(torch.flatten(y1>y2)) / len(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5965b5-3b89-4669-9426-2328f7e5bd00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.5258\n",
      "Epoch [10/50], Loss: 0.4616\n",
      "Epoch [15/50], Loss: 0.4577\n",
      "Epoch [20/50], Loss: 0.4557\n",
      "Epoch [25/50], Loss: 0.4545\n",
      "Epoch [30/50], Loss: 0.4539\n",
      "Epoch [35/50], Loss: 0.4537\n",
      "Epoch [40/50], Loss: 0.4535\n",
      "Epoch [45/50], Loss: 0.4534\n",
      "Epoch [50/50], Loss: 0.4534\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = CustomLoss()  \n",
    "optim = torch.optim.AdamW(rm.parameters())\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  y1 = rm(torch.stack([stot(pairwise_data[i][0]).argmax(-1) for i in range(len(pairwise_data))]).long().to(device))\n",
    "  y2 = rm(torch.stack([stot(pairwise_data[i][1]).argmax(-1) for i in range(len(pairwise_data))]).long().to(device))\n",
    "\n",
    "\n",
    "  # # Compute the loss\n",
    "  loss = criterion(y1, y2)\n",
    "\n",
    "  # Backward pass and optimization\n",
    "  \n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  optim.zero_grad()\n",
    "\n",
    "  # Print the loss every few epochs\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "      print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7130516-99b6-4f8b-9e95-a305d9fd2050",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9943, device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  y1 = rm(torch.stack([stot(pairwise_data[i][0]).argmax(-1) for i in range(len(pairwise_data))]).long().to(device))\n",
    "  y2 = rm(torch.stack([stot(pairwise_data[i][1]).argmax(-1) for i in range(len(pairwise_data))]).long().to(device))\n",
    "\n",
    "  sum(torch.flatten(y1>y2)) / len(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41908f40-d2f4-4227-aa11-d0970cf844b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 3: Train a policy gradient model to align the language model with your preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40ca1d8-5fad-4090-b270-dc6d0fbabc50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "state_action_pairs = [states[i] + actions[i] for i in range(len(states))]\n",
    "rewards = rm(torch.stack([stot(pair).argmax(-1) for pair in state_action_pairs]).long().to(device)).to(device)\n",
    "\n",
    "states_vector = torch.stack([stot(state).argmax(-1) for state in states]).long().to(device)\n",
    "actions_vector = torch.stack([stot(action).argmax(-1) for action in actions]).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7bd367c-50fd-4de2-bc8c-cf755c6084ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['^^^^^t', '^^^^ti', '^^^tim', '^^time', '^time$', '^^^^^s', '^^^^si', '^^^sit', '^^site', '^site$']\n",
      "tensor([[1.1073e-04],\n",
      "        [1.2602e-04],\n",
      "        [6.5515e-01],\n",
      "        [9.9982e-01],\n",
      "        [9.9997e-01],\n",
      "        [1.0548e-04],\n",
      "        [1.4360e-04],\n",
      "        [7.3108e-01],\n",
      "        [9.9988e-01],\n",
      "        [9.9996e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(state_action_pairs[0:10])\n",
    "print(rewards[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb12f6d3-4600-456c-95d1-c9c3857ddff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "policy_gradient_nn = copy.deepcopy(model)\n",
    "\n",
    "policy_gradient_optim = torch.optim.AdamW(policy_gradient_nn.parameters())\n",
    "policy_gradient_optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768197b7-3685-4b62-b3fd-d5ad21d0ef69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples from lm\n",
      "samples:  ['ptymeutments', 'tudivery', 'fourt', 'calling', 'diabor', 'bulles', 'crins', 'miltan', 'ahictd', 'gating', 'taucktlians', 'acceing', 'hargog', 'homelo', 'lix', 'enemine', 'alestate', 'lolled', 'kiche', 'cub']\n",
      "avg word length:  6.713\n",
      "num valid english words (out of 1000): 90\n",
      "\n",
      "STEP: 0\n",
      "samples:  ['neek', 'sminder', 'tasel', 'diag', 'sirm', 'tramaging', 'caread', 'steeved', 'alb', 'gook', 'brisons', 'robs', 'buttansh', 'stated', 'home', 'sandbance', 'pens', 'pensexuasumptio', 'proces', 'sovers']\n",
      "avg word length:  6.649\n",
      "num valid english words (out of 1000): 94\n",
      "\n",
      "STEP: 1\n",
      "samples:  ['enerating', 'desited', 'reprintality', 'solumssy', 'morc', 'cont', 'oppanc', 'surtime', 'mov', 'plantly', 'breeldlake', 'sarriemis', 'treat', 'fortion', 'dring', 'clistial', 'speegra', 'bst', 'struve', 'begense']\n",
      "avg word length:  6.371\n",
      "num valid english words (out of 1000): 125\n",
      "\n",
      "STEP: 2\n",
      "samples:  ['sitorated', 'pay', 'hosting', 'phanu', 'wit', 'plotor', 'cations', 'drands', 'dighas', 'dornings', 'idgliam', 'con', 'unly', 'mearing', 'punner', 'vinger', 'haf', 'mid', 'hampie', 'marthlaral']\n",
      "avg word length:  6.227\n",
      "num valid english words (out of 1000): 139\n",
      "\n",
      "STEP: 3\n",
      "samples:  ['depend', 'cradley', 'culc', 'modacion', 'traislm', 'enter', 'mirculation', 'suppositional', 'cons', 'decast', 'dels', 'lusted', 'tequationing', 'clet', 'baseltires', 'socia', 'seeding', 'cable', 'jonner', 'cruns']\n",
      "avg word length:  6.042\n",
      "num valid english words (out of 1000): 136\n",
      "\n",
      "STEP: 4\n",
      "samples:  ['dam', 'invy', 'yee', 'arad', 'now', 'brio', 'vanu', 'ablead', 'edtn', 'camparited', 'aur', 'impanifies', 'charc', 'conse', 'providating', 'ret', 'vingatuding', 'hegacy', 'heter', 'ply']\n",
      "avg word length:  5.753\n",
      "num valid english words (out of 1000): 138\n",
      "\n",
      "STEP: 5\n",
      "samples:  ['hit', 'holst', 'chicking', 'sime', 'treadly', 'attent', 'tatue', 'mere', 'pleviterne', 'coneciencem', 'balp', 'she', 'spc', 'urgaes', 'ludee', 'phire', 'recenging', 'lawsh', 'stackhg', 'dolk']\n",
      "avg word length:  5.661\n",
      "num valid english words (out of 1000): 153\n",
      "\n",
      "STEP: 6\n",
      "samples:  ['sesure', 'proc', 'weas', 'traill', 'land', 'loadh', 'deport', 'hir', 'anners', 'study', 'yardy', 'jen', 'guiliad', 'sonikin', 'eat', 'potrum', 'tut', 'mincorest', 'dam', 'mit']\n",
      "avg word length:  5.572\n",
      "num valid english words (out of 1000): 147\n",
      "\n",
      "STEP: 7\n",
      "samples:  ['mod', 'sub', 'med', 'fum', 'cime', 'caustmination', 'diviforis', 'heatur', 'suith', 'spid', 'showaes', 'boolla', 'fom', 'cen', 'shiple', 'swidt', 'thes', 'dras', 'froses', 'clo']\n",
      "avg word length:  5.298\n",
      "num valid english words (out of 1000): 166\n",
      "\n",
      "STEP: 8\n",
      "samples:  ['photogn', 'semb', 'fit', 'quaph', 'b', 'inqui', 'rehy', 'suplice', 'gusburn', 'ingo', 'cubity', 'sto', 'bul', 'finhmeni', 'threm', 'advusetion', 'novy', 'mark', 'blohold', 'height']\n",
      "avg word length:  5.311\n",
      "num valid english words (out of 1000): 148\n",
      "\n",
      "STEP: 9\n",
      "samples:  ['edons', 'mark', 'ruso', 'cape', 'empricieating', 'suspus', 'restream', 'chrfast', 'muas', 'codgabet', 'reg', 'class', 'bla', 'mus', 'dath', 'nating', 'ducan', 'tesus', 'regal', 'cele']\n",
      "avg word length:  5.05\n",
      "num valid english words (out of 1000): 169\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"samples from lm\")\n",
    "\n",
    "gen_samples(policy_gradient_nn)\n",
    "\n",
    "for step in range(10):\n",
    "  rewards = rm(torch.stack([stot(pair).argmax(-1) for pair in state_action_pairs]).long().to(device)).to(device)\n",
    "  states_vector = torch.stack([stot(state).argmax(-1) for state in states]).long().to(device)\n",
    "  actions_vector = torch.stack([stot(action).argmax(-1) for action in actions]).long().to(device)\n",
    "\n",
    "  logits = policy_gradient_nn(states_vector)\n",
    "\n",
    "  log_prob_dist = torch.nn.functional.log_softmax(logits, -1)\n",
    "  log_probs = log_prob_dist[torch.arange(len(actions_vector)), actions_vector]\n",
    "\n",
    "  #policy gradient\n",
    "  loss = -(rewards * log_probs).mean() * 1e-8 # low learning rate to make the algorithm stable\n",
    "\n",
    "\n",
    "  loss.backward()\n",
    "  policy_gradient_optim.step()\n",
    "  policy_gradient_optim.zero_grad()\n",
    "\n",
    "  print()\n",
    "  print(f\"STEP: {step}\")\n",
    "  gen_samples(policy_gradient_nn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed6b262c-b6c3-466c-8943-7003b27fff64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 4: Train a proximal policy optimisation model to align the language model with your preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5393978c-a602-4803-b09b-affb0d0ea381",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rl_model = copy.deepcopy(model)\n",
    "ref_model = copy.deepcopy(model)\n",
    "\n",
    "rl_optim = torch.optim.Adam(rl_model.parameters())\n",
    "rl_optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ce0b3a-1995-4a9e-8f7f-b578c6d23ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples:  ['siderights', 'portinual', 'salveres', 'proce', 'ailtandablence', 'part', 'rake', 'achuage', 'sald', 'happting', 'genums', 'mati', 'involve', 'demonsion', 'afft', 'regory', 'uttack', 'lema', 'ner', 'neeptopo']\n",
      "avg word length:  6.634\n",
      "num valid english words (out of 1000): 112\n",
      "samples:  ['dovols', 'goderaile', 'chineshing', 'structoors', 'pavillanda', 'incousesssbuhed', 'medriolsor', 'holving', 'libre', 'recerves', 'beaccivition', 'baus', 'futs', 'gongsten', 'ponshing', 'lid', 'subscrretion', 'assications', 'call', 'pspecian']\n",
      "avg word length:  6.581\n",
      "num valid english words (out of 1000): 112\n",
      "samples:  ['cabylactor', 'turna', 'bunger', 'prove', 'transh', 'nowte', 'hunced', 'tub', 'unsp', 'earling', 'rese', 'promaty', 'explor', 'finner', 'brang', 'ben', 'reveluding', 'coller', 'compos', 'site']\n",
      "avg word length:  6.317\n",
      "num valid english words (out of 1000): 117\n",
      "samples:  ['tocelly', 'mus', 'heasplices', 'inn', 'carely', 'annon', 'schelds', 'delection', 'cootherbburaws', 'now', 'atmily', 'ter', 'batter', 'soft', 'scipfies', 'hasod', 'hylic', 'down', 'sundated', 'aco']\n",
      "avg word length:  6.066\n",
      "num valid english words (out of 1000): 144\n",
      "samples:  ['lecessless', 'finion', 'pate', 'hurs', 'allungentions', 'unzy', 'part', 'thri', 'enshops', 'exprome', 'fick', 'casses', 'fauntion', 'resush', 'blog', 'thave', 'erromer', 'priets', 'sublions', 'tracts']\n",
      "avg word length:  5.923\n",
      "num valid english words (out of 1000): 137\n",
      "samples:  ['questing', 'gapar', 'photle', 'suprits', 'stam', 'smsurbbility', 'reekly', 'hing', 'chrying', 'treved', 'coarner', 'liken', 'shoure', 'clopr', 'lionbles', 'cango', 'funct', 'cren', 'terong', 'gabs']\n",
      "avg word length:  5.737\n",
      "num valid english words (out of 1000): 141\n",
      "samples:  ['ursuatest', 'projicten', 'initigale', 'wreminstly', 'beve', 'drustial', 'gst', 'dam', 'bruke', 'consuits', 'chusel', 'dive', 'satt', 'trades', 'dop', 'inxture', 'seccessions', 'now', 'strums', 'resk']\n",
      "avg word length:  5.674\n",
      "num valid english words (out of 1000): 152\n",
      "samples:  ['infe', 'furg', 'asspo', 'spreases', 'writory', 'udy', 'ungo', 'ring', 'untse', 'lana', 'scad', 'nowl', 'rpronic', 'unticipts', 'volvh', 'toccoured', 'sony', 'tutisional', 'helchee', 'sph']\n",
      "avg word length:  5.504\n",
      "num valid english words (out of 1000): 152\n",
      "samples:  ['tharior', 'fong', 'ven', 'rergest', 'dem', 'brugue', 'enjaq', 'actic', 'intertal', 'obs', 'dowbai', 'dri', 'mid', 'repiriby', 'notute', 'disican', 'ettlog', 'decs', 'gre', 'doll']\n",
      "avg word length:  5.342\n",
      "num valid english words (out of 1000): 150\n",
      "samples:  ['charm', 'publitan', 'all', 'lock', 'phore', 'tham', 'deening', 'tann', 'tfood', 'phiren', 'red', 'blogited', 'rener', 'ban', 'bung', 'chor', 'alby', 'brid', 'entancy', 'hetect']\n",
      "avg word length:  5.12\n",
      "num valid english words (out of 1000): 156\n",
      "samples:  ['binkles', 'udgen', 'shop', 'heast', 'nemplat', 'thriz', 'dan', 'coun', 'psg', 'hel', 'palet', 'launct', 'netic', 'tra', 'conce', 'ney', 'sol', 'rece', 'chi', 'pro']\n",
      "avg word length:  5.035\n",
      "num valid english words (out of 1000): 153\n",
      "samples:  ['obe', 'hab', 'institing', 'octor', 'grost', 'merring', 'incephed', 'melim', 'gro', 'staikes', 'day', 'untics', 'davin', 'budger', 'tran', 'tui', 'swo', 'ungar', 'usb', 'junney']\n",
      "avg word length:  4.852\n",
      "num valid english words (out of 1000): 169\n",
      "samples:  ['rad', 'rop', 'mating', 'get', 'tudine', 'victon', 'descreash', 'p', 'nio', 'unad', 'tarclas', 'new', 'senque', 'anno', 'muz', 'loom', 'com', 'muzliad', 'munussion', 'suba']\n",
      "avg word length:  4.701\n",
      "num valid english words (out of 1000): 167\n",
      "samples:  ['acm', 'thuth', 'elko', 'playpent', 'veter', 'cur', 'name', 'dif', 'observa', 'hter', 'usdo', 'phos', 'bsolta', 'mum', 'mpate', 'cossen', 'hethlo', 'bive', 'attunt', 'reg']\n",
      "avg word length:  4.596\n",
      "num valid english words (out of 1000): 160\n",
      "samples:  ['heth', 'trible', 'shiea', 'flos', 'run', 'suflitive', 'than', 'uncl', 'jas', 'joybs', 'plyy', 'chc', 'chick', 'a', 'suzla', 'nups', 'pubony', 'sharitst', 'flows', 'snix']\n",
      "avg word length:  4.408\n",
      "num valid english words (out of 1000): 166\n",
      "samples:  ['im', 'han', 'mem', 'terch', 'broons', 'chekle', 'undats', 'diftion', 'mem', 'frimna', 'sho', 'pidne', 'mel', 'inder', 'mob', 'cyer', 'phare', 'ham', 'bed', 'enth']\n",
      "avg word length:  4.361\n",
      "num valid english words (out of 1000): 161\n",
      "samples:  ['buffal', 'jccustit', 'resc', 'drase', 'heabilit', 'upro', 'roo', 'helies', 'train', 'bum', 'thient', 'elista', 'missions', 'arron', 'proses', 'temp', 'mse', 'reg', 'nive', 'tunne']\n",
      "avg word length:  4.033\n",
      "num valid english words (out of 1000): 173\n",
      "samples:  ['dim', 'attraches', 'tra', 'dad', 'wor', 'mat', 'ach', 'ult', 'dots', 'egie', 'shok', 'driens', 'dan', 'subs', 'seage', 'tuntra', 'sty', '', 'des', '']\n",
      "avg word length:  3.957\n",
      "num valid english words (out of 1000): 166\n",
      "samples:  ['diple', 'chick', 'her', 'nen', 'hossel', 'hylas', 'sum', 'faid', 'selia', 'mends', 'rm', 'trick', 'adv', 'det', 'bulling', 'mem', 'g', 'new', 'ph', 'brio']\n",
      "avg word length:  3.917\n",
      "num valid english words (out of 1000): 154\n",
      "samples:  ['smoke', 'inved', 'hoo', 'end', 'ettal', 'met', 'ph', '', 'shar', 'sufficic', 'ret', 'det', 'mod', 'pol', 'd', 'hee', 'imp', 'chick', 'ments', 'sups']\n",
      "avg word length:  3.642\n",
      "num valid english words (out of 1000): 138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for step in range(15):\n",
    "  rewards = rm(torch.stack([stot(pair).argmax(-1) for pair in state_action_pairs]).long().to(device)).to(device)\n",
    "  states_vector = torch.stack([stot(state).argmax(-1) for state in states]).long().to(device)\n",
    "  actions_vector = torch.stack([stot(action).argmax(-1) for action in actions]).long().to(device)\n",
    "\n",
    "\n",
    "  logits = rl_model(states_vector)\n",
    "  log_probs = torch.nn.functional.log_softmax(logits, -1)[torch.arange(len(actions_vector)), actions_vector]\n",
    "\n",
    "  with torch.no_grad():\n",
    "    ref_log_probs = torch.nn.functional.log_softmax(ref_model(states_vector), -1)[torch.arange(len(actions_vector)), actions_vector]\n",
    "\n",
    "  ratio = log_probs - ref_log_probs\n",
    "  ratio = ratio.exp()\n",
    "\n",
    "  ppo_loss1 = rewards * ratio\n",
    "  ppo_loss2 = rewards * torch.clamp(ratio, .8, 1.2)\n",
    "\n",
    "  loss = -torch.min(ppo_loss1, ppo_loss2).mean()\n",
    "\n",
    "  #policy gradient\n",
    "  loss = -(rewards * log_probs).mean() * 1e-8\n",
    "\n",
    "  gen_samples(rl_model) \n",
    "\n",
    "  loss.backward()\n",
    "  rl_optim.step()\n",
    "  rl_optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf2f746b-6d1c-4468-81f0-4c0ad280160a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "RLHF  (from scratch)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
